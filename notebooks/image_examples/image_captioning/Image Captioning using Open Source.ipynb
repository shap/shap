{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explaining Image Captioning (Image to Text) using Open Source Image Captioning Model and Partition Explainer\n",
    "\n",
    "This notebook demonstrates how to use SHAP for explaining output of image captioning models i.e. given an image, model outputs a caption for the image. \n",
    "\n",
    "Here, we are using a pre-trained open source model from https://github.com/ruotianluo/ImageCaptioning.pytorch to get image captions. All pre-trained models are available at https://github.com/ruotianluo/ImageCaptioning.pytorch/blob/master/MODEL_ZOO.md. Particularly, this notebook uses the model trained with ResNet101 features linked under \"FC+new_self_critical\" model & metrics https://drive.google.com/open?id=1OsB_jLDorJnzKz6xsOfk1n493P3hwOP0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations \n",
    "\n",
    "1. To explain image captions, we are segmenting images along axes (i.e. super pixels/partitions of halves, quarters, eights...); An alternate approach/future improvement could be to semantically segment images instead of axis-aligned partitioning and produce SHAP explanations using segments, instead of super pixels. https://github.com/shap/shap/issues/1738 \n",
    "\n",
    "2. We are using transformer language model (ex. distilbart) to do alignment scoring between given image and masked image captions, assuming an external model is a good surrogate for the original captioning model's language head. By using the captioning model's own language head, we could eliminate this assumption and remove the dependency. (ex. refer to text2text notebook examples). For more details, refer to the \"Load language model and tokenizer\" section below. https://github.com/shap/shap/issues/1739 \n",
    "\n",
    "3. The more evaluations used to generate explanations, longer it takes for SHAP to run. But, increasing the number of evaluations increases the granularity of the explanations (300-500 evaluations often produce detailed maps, but fewer or more are also often reasonable). Refer to \"Create an explainer object using wrapped model and image masker\" section below for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up open source model\n",
    "\n",
    "##### Note: It is important to follow set up instructions exactly as given below to ensure notebook runs.\n",
    "\n",
    "1. Clone https://github.com/ruotianluo/ImageCaptioning.pytorch repo. In a terminal type: 'git clone https://github.com/ruotianluo/ImageCaptioning.pytorch'.\n",
    "\n",
    "2. Change below PREFIX variable to have absolute path of your ImageCaptioning.pytorch folder. This is an important step to ensure all file paths are accessed correctly.\n",
    "\n",
    "3. Download the following files and place them in the folders as given:\n",
    "    1. \"model-best.pth\": download from here https://drive.google.com/drive/folders/1OsB_jLDorJnzKz6xsOfk1n493P3hwOP0  and place in the cloned directory.\n",
    "    2. \"infos_fc_nsc-best.pkl\": download from here https://drive.google.com/drive/folders/1OsB_jLDorJnzKz6xsOfk1n493P3hwOP0 and place in the cloned directory\n",
    "    3. \"resnet101\": download from here https://drive.google.com/drive/folders/0B7fNdx_jAqhtbVYzOURMdDNHSGM and place in the cloned directory under 'data/imagenet_weights' folder. Create 'imagenet_weights' folder under 'data' directory if it doesn't exist. \n",
    "\n",
    "4. In a terminal, navigate to the cloned folder and type \"python -m pip install -e .\" or in a cell in jupyter notebook \"!python -m pip install -e .\" to install the module. \n",
    "\n",
    "5. Restart and clear kernel output to run this notebook. \n",
    "\n",
    "6. *Optional*: After running cells below in \"Load sample data\" section which loads sample data and creates a './test_images/' folder, try \"python tools/eval.py --model model-best.pth --infos_path infos_fc_nsc-best.pkl --image_folder test_images --num_images 10\" command in a terminal to verify that the installation was succesful. If it fails, please install any missing packages. ex. if 'lmdbdict' package is missing, try installing using \"pip install git+https://github.com/ruotianluo/lmdbdict.git\". If captions are shown in terminal output, then the installation was successful. \n",
    "\n",
    "    ##### Note: If these commands are being tested in a jupyter notebook, append ! in front of the commands. ex. \"!python -m pip install -e .\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load sample data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shap\n",
    "from shap.utils.image import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change PREFIX to have absolute path of cloned directory of ImageCaptioning.pytorch \n",
    "PREFIX = r\"<place full path to the cloned directory of ImageCaptioning.pytorch>/ImageCaptioning.pytorch\"\n",
    "os.chdir(PREFIX)\n",
    "\n",
    "# directory of images to be explained \n",
    "DIR = './test_images/' \n",
    "# creates or empties directory if it already exists\n",
    "make_dir(DIR)\n",
    "add_sample_images(DIR)\n",
    "\n",
    "# directory for saving masked images\n",
    "DIR_MASKED = './masked_images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import captioning\n",
    "import captioning.models as models\n",
    "import captioning.utils.eval_utils as eval_utils\n",
    "import captioning.utils.misc as utils\n",
    "import captioning.modules.losses as losses\n",
    "from captioning.data.dataloader import *\n",
    "from captioning.data.dataloaderraw import *\n",
    "import gc\n",
    "import sys\n",
    "import torch\n",
    "from transformers import AutoTokenizer,AutoModelForSeq2SeqLM\n",
    "\n",
    "# to suppress verbose output from open source model\n",
    "from contextlib import contextmanager\n",
    "@contextmanager\n",
    "def suppress_stdout():\n",
    "    with open(os.devnull, \"w\") as devnull:\n",
    "        old_stdout = sys.stdout\n",
    "        sys.stdout = devnull\n",
    "        old_stderr = sys.stderr\n",
    "        sys.stderr = devnull\n",
    "        try:  \n",
    "            yield\n",
    "        finally:\n",
    "            sys.stdout = old_stdout\n",
    "            sys.stderr = old_stderr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting captions using open source model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptioningPyTorchModel:\n",
    "    \"\"\"\n",
    "    Wrapper class to get image captions using Resnet model from setup above. \n",
    "    Note: This class is being used instead of tools/eval.py to get predictions (captions). \n",
    "    To get more context for this class, please refer to tools/eval.py file. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_path, infos_path, cnn_model = \"resnet101\", device = \"cuda\"):\n",
    "        \"\"\"\n",
    "        Initializing the class by loading torch model and vocabulary at path given and using Resnet weights stored in data/imagenet_weights. \n",
    "        This is done to speeden the process of getting image captions and avoid loading the model every time captions are needed. \n",
    "        Parameters\n",
    "        ----------\n",
    "        model_path  : pre-trained model path\n",
    "        infos_path  : pre-trained infos (vocab) path\n",
    "        cnn_model   : resnet model weights to use; options: \"resnet101\" (default), \"resnet152\"\n",
    "        device      : \"cpu\" or \"cuda\" (default)\n",
    "        \"\"\"\n",
    "\n",
    "        # load infos\n",
    "        with open(infos_path, 'rb') as f:\n",
    "            infos = utils.pickle_load(f)\n",
    "        opt = infos['opt']\n",
    "        \n",
    "        # setup the model\n",
    "        opt.model = model_path\n",
    "        opt.cnn_model = cnn_model\n",
    "        opt.device = device\n",
    "        opt.vocab = infos['vocab'] # ix -> word mapping\n",
    "        model = models.setup(opt)\n",
    "        del infos\n",
    "        del opt.vocab\n",
    "        model.load_state_dict(torch.load(opt.model, map_location='cpu'))\n",
    "        model.to(opt.device)\n",
    "        model.eval()\n",
    "        crit = losses.LanguageModelCriterion()\n",
    "        \n",
    "        # setup class variables for call function\n",
    "        self.opt = opt\n",
    "        self.model = model\n",
    "        self.crit = crit\n",
    "        self.infos_path = infos_path\n",
    "\n",
    "        # free memory\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "\n",
    "    def __call__(self, image_folder, batch_size):\n",
    "        \"\"\"\n",
    "        Function to get captions for images placed in image_folder. \n",
    "        Parameters\n",
    "        ----------\n",
    "        image_folder: folder of images for which captions are needed\n",
    "        batch_size  : number of images to be evaluated at once\n",
    "        Output\n",
    "        -------\n",
    "        captions    : list of captions for images in image_folder (will return a string if there is only one image in folder)\n",
    "        \"\"\"\n",
    "\n",
    "        # setting eval options\n",
    "        opt = self.opt\n",
    "        opt.batch_size = batch_size\n",
    "        opt.image_folder = image_folder\n",
    "        opt.coco_json = \"\"\n",
    "        opt.dataset = opt.input_json\n",
    "        opt.verbose_loss = 0\n",
    "        opt.verbose = False\n",
    "        opt.dump_path = 0\n",
    "        opt.dump_images = 0\n",
    "        opt.num_images = -1\n",
    "        opt.language_eval = 0\n",
    "        \n",
    "        # loading vocab\n",
    "        with open(self.infos_path, 'rb') as f:\n",
    "            infos = utils.pickle_load(f)\n",
    "        opt.vocab = infos['vocab']\n",
    "            \n",
    "        # creating Data Loader instance to load images\n",
    "        if len(opt.image_folder) == 0:\n",
    "            loader = DataLoader(opt)\n",
    "        else:\n",
    "            loader = DataLoaderRaw({'folder_path': opt.image_folder, \n",
    "                                    'coco_json': opt.coco_json,\n",
    "                                    'batch_size': opt.batch_size,\n",
    "                                    'cnn_model': opt.cnn_model})\n",
    "            \n",
    "        # when evaluating using provided pretrained model, vocab may be different from what is in cocotalk.json. \n",
    "        # hence, setting vocab from infos file.\n",
    "        loader.dataset.ix_to_word = opt.vocab\n",
    "        del infos\n",
    "        del opt.vocab\n",
    "        \n",
    "        # getting caption predictions \n",
    "        _, split_predictions, _ = eval_utils.eval_split(self.model, self.crit, loader, vars(opt))\n",
    "        captions = []\n",
    "        for line in split_predictions:\n",
    "            captions.append(line['caption'])\n",
    "        \n",
    "        # free memory\n",
    "        del loader\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "            \n",
    "        return captions if len(captions) > 1 else captions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create instance of ImageCaptioningPyTorchModel\n",
    "osmodel = ImageCaptioningPyTorchModel(model_path = \"model-best.pth\",\n",
    "                        infos_path = \"infos_fc_nsc-best.pkl\", \n",
    "                        cnn_model = \"resnet101\", \n",
    "                        device = \"cpu\")\n",
    "\n",
    "# create function to get caption using model created above\n",
    "def get_caption(model, image_folder, batch_size):\n",
    "    return model(image_folder, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "\n",
    "'./test_images/' is the folder of images that will be explained. './test_images/' directory has been created for you and sample images needed to replicate examples shown in the notebook are already placed in the directory. \n",
    "\n",
    "### Note: **Replace or add images that you would like to be explained(tested) in the './test_images/' folder.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checks if test images folder exists and if it has any files\n",
    "if not is_empty(DIR):\n",
    "\n",
    "    X = []\n",
    "    print(\"Loading data...\")\n",
    "    files = [f for f in os.listdir(DIR) if os.path.isfile(os.path.join(DIR, f))]\n",
    "    for file in files:\n",
    "        path_to_image = os.path.join(DIR, file)\n",
    "        print(\"Loading image:\", file)\n",
    "        X.append(load_image(path_to_image))\n",
    "    with suppress_stdout():\n",
    "        captions = get_caption(osmodel, \"test_images\", 5)\n",
    "    if len(X) > 1:\n",
    "       print(\"\\nCaptions are...\", *captions, sep = \"\\n\")\n",
    "    else:\n",
    "        print(\"\\nCaption is...\", captions)\n",
    "    print(\"\\nNumber of images in test dataset:\", len(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load language model and tokenizer\n",
    "\n",
    "Transformer Language Model 'distilbart' and tokenizer are being used here to tokenize the image caption. This makes the image to text scenario similar to a multi-class problem. 'distilbart' is used to do alignment scoring between the original image caption and masked image captions being generated i.e. how does the probability of getting the original image caption change when the context of a masked image caption is given? (a.k.a. we are teacher forcing 'distilbart' to always produce the original image caption for the masked images and getting change in logits for each tokenized word in the caption as part of the process).\n",
    "\n",
    "**Note**: We are using 'distilbart' here because during experimentation process we found it to give the most meaningful explanations for images. We have compared with other language models such as 'openaigpt' and 'distilgpt2'. Please feel free to explore with other language models of your choice and compare the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load transformer language model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sshleifer/distilbart-xsum-12-6\")\n",
    "model =  AutoModelForSeq2SeqLM.from_pretrained(\"sshleifer/distilbart-xsum-12-6\").cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an explainer object using wrapped model and image masker\n",
    "\n",
    "Various options for explainer object to experiment with:\n",
    "\n",
    "1. **mask_value**      : Image masker uses an inpainting technqiue by default for masking (i.e. mask_value = \"inpaint_ns\"). There are alternate masking options available for blurring/inpainting  such as \"inpaint_telea\" and \"blur(kernel_xsize, kernel_xsize)\". Note: Different explanations can be generated by different masking options.\n",
    "\n",
    "2. **max_evals**       : Number of evaluations done of the underlying model to get SHAP values. Recommended number of evaluations is 300-500 to get explanations with meaningful granularity of super pixels. More the number of evaluations, more the granularity but also increases run-time. Default is set to 300 evals. \n",
    "\n",
    "3. **batch_size**      : Number of masked images to be evaluated at once. Default size is set to 50. \n",
    "\n",
    "4. **fixed_context**   : Masking technqiue used to build partition tree with options of '0', '1' or 'None'. 'fixed_context = None' is the best option to generate meaningful results but it is relatively slower than fixed_context = 0 or 1 because it generates a full partition tree. Default option is set to 'None'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting values for logging/tracking variables\n",
    "make_dir(DIR_MASKED)\n",
    "image_counter = 0 \n",
    "mask_counter = 0 \n",
    "\n",
    "\n",
    "# define function f which takes input (masked image) and returns caption for it \n",
    "def f(x): \n",
    "    global mask_counter\n",
    "    \n",
    "    # emptying masked images directory \n",
    "    make_dir(DIR_MASKED)\n",
    "\n",
    "    # saving masked array of RGB values as an image in masked_images directory\n",
    "    path_to_image = os.path.join(DIR_MASKED, \"{0}_{1}.png\".format(image_counter, mask_counter))\n",
    "    save_image(x, path_to_image) \n",
    "    \n",
    "    # getting caption of masked image \n",
    "    with suppress_stdout():\n",
    "        caption = get_caption(osmodel, \"masked_images\", 5) \n",
    "    mask_counter += 1 \n",
    "    \n",
    "    return caption\n",
    "\n",
    "# function to take a list of images and parameters such as masking option, max evals etc. and return shap_values_objects\n",
    "def run_masker(X, mask_value = \"inpaint_ns\", max_evals = 300, batch_size = 50, fixed_context = None):\n",
    "    \"\"\"\n",
    "    Function to take a list of images and parameters such max evals etc. and return shap explanations (shap_values) for test images(X).\n",
    "    Paramaters\n",
    "    ----------\n",
    "    X               : list of images which need to be explained\n",
    "    mask_value      : various masking options for blurring/inpainting such as \"inpaint_ns\", \"inpaint_telea\" and \"blur(pixel_size, pixel_size)\"\n",
    "    max_evals       : number of evaluations done of the underlying model to get SHAP values \n",
    "    batch_size      : number of masked images to be evaluated at once \n",
    "    fixed_context   : masking technqiue used to build partition tree with options of '0', '1' or 'None'\n",
    "    Output\n",
    "    ------\n",
    "    shap_values_list: list of shap_values objects generated for the images\n",
    "    \"\"\"\n",
    "    global image_counter\n",
    "    global mask_counter\n",
    "    shap_values_list = []\n",
    "\n",
    "    for index in range(len(X)): \n",
    "\n",
    "        # define a masker that is used to mask out partitions of the input image based on mask_value option\n",
    "        masker = shap.maskers.Image(mask_value, X[index].shape)\n",
    "\n",
    "        # wrap model with TeacherForcingLogits class\n",
    "        wrapped_model = shap.models.TeacherForcingLogits(f, similarity_model=model, similarity_tokenizer=tokenizer)\n",
    "\n",
    "        # build a partition explainer with wrapped_model and image masker\n",
    "        explainer = shap.Explainer(wrapped_model, masker) \n",
    "\n",
    "        # compute SHAP values - here we use max_evals no. of evaluations of the underlying model to estimate SHAP values\n",
    "        shap_values = explainer(np.array(X[index:index+1]), max_evals = max_evals, batch_size = batch_size, fixed_context = fixed_context)\n",
    "        shap_values_list.append(shap_values)\n",
    "\n",
    "        # output plot\n",
    "        shap_values.output_names[0] = [word.replace('Ä ', '') for word in shap_values.output_names[0]]\n",
    "        shap.image_plot(shap_values)\n",
    "\n",
    "        # setting values for next iterations\n",
    "        mask_counter = 0\n",
    "        image_counter += 1\n",
    "    \n",
    "    return shap_values_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP explanation for test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP explanation using masking option \"blur(pixel_size, pixel_size)\" for blurring\n",
    "shap_values = run_masker(X, mask_value=\"blur(56,56)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP explanation using masking option \"inpaint_telea\" for inpainting\n",
    "shap_values = run_masker(X[3:4], mask_value= \"inpaint_telea\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('shap': conda)",
   "metadata": {
    "interpreter": {
     "hash": "697f89d28448171571004865d8d182b75b074b51755e0472a9232d60cc32586e"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
