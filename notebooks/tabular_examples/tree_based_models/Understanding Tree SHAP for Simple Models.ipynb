{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Tree SHAP for Simple Models\n",
    "\n",
    "The SHAP value for a feature is the average change in model output by conditioning on that feature when introducing features one at a time over all feature orderings. While this is easy to state, it is challenging to compute. So this notebook is meant to give a few simple examples where we can see how this plays out for very small trees. For arbitrary large trees it is very hard to intuitively guess these values by looking at the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import shap\n",
    "import numpy as np\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single split example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build data\n",
    "N = 100\n",
    "M = 4\n",
    "X = np.zeros((N,M))\n",
    "X.shape\n",
    "y = np.zeros(N)\n",
    "X[:N//2, 0] = 1\n",
    "y[:N//2] = 1\n",
    "\n",
    "# fit model\n",
    "single_split_model = sklearn.tree.DecisionTreeRegressor(max_depth=1)\n",
    "single_split_model.fit(X, y)\n",
    "\n",
    "# draw model\n",
    "dot_data = sklearn.tree.export_graphviz(single_split_model, out_file=None, filled=True, rounded=True, special_characters=True)  \n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explain the model\n",
    "\n",
    "Note that the bias term is the expected output of the model over the training dataset (0.5). The SHAP value for features not used in the model is always 0, while for $x_0$ it is just the difference between the expected value and the output of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = [np.ones(M), np.zeros(M)]\n",
    "for x in xs:\n",
    "    print()\n",
    "    print(\"          x =\", x)\n",
    "    print(\"shap_values =\", shap.TreeExplainer(single_split_model).shap_values(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two feature AND example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build data\n",
    "N = 100\n",
    "M = 4\n",
    "X = np.zeros((N,M))\n",
    "X.shape\n",
    "y = np.zeros(N)\n",
    "X[:1 * N//4, 1] = 1\n",
    "X[:N//2, 0] = 1\n",
    "X[N//2:3 * N//4, 1] = 1\n",
    "y[:1 * N//4] = 1\n",
    "\n",
    "# fit model\n",
    "and_model = sklearn.tree.DecisionTreeRegressor(max_depth=2)\n",
    "and_model.fit(X, y)\n",
    "\n",
    "# draw model\n",
    "dot_data = sklearn.tree.export_graphviz(and_model, out_file=None, filled=True, rounded=True, special_characters=True)  \n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explain the model\n",
    "\n",
    "Note that the bias term is the expected output of the model over the training dataset (0.25). The SHAP value for features not used in the model is always 0, while for $x_0$ and $x_1$ it is just the difference between the expected value and the output of the model split equally between them (since they equally contribute to the AND function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = [np.ones(M), np.zeros(M)]\n",
    "for x in xs:\n",
    "    print()\n",
    "    print(\"          x =\", x)\n",
    "    print(\"shap_values =\", shap.TreeExplainer(and_model).shap_values(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two feature OR example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build data\n",
    "N = 100\n",
    "M = 4\n",
    "X = np.zeros((N,M))\n",
    "X.shape\n",
    "y = np.zeros(N)\n",
    "X[:N//2, 0] = 1\n",
    "X[:1 * N//4, 1] = 1\n",
    "X[N//2:3 * N//4, 1] = 1\n",
    "y[:N//2] = 1\n",
    "y[N//2:3 * N//4] = 1\n",
    "\n",
    "# fit model\n",
    "or_model = sklearn.tree.DecisionTreeRegressor(max_depth=2)\n",
    "or_model.fit(X, y)\n",
    "\n",
    "# draw model\n",
    "dot_data = sklearn.tree.export_graphviz(or_model, out_file=None, filled=True, rounded=True, special_characters=True)  \n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explain the model\n",
    "\n",
    "Note that the bias term is the expected output of the model over the training dataset (0.75). The SHAP value for features not used in the model is always 0, while for $x_0$ and $x_1$ it is just the difference between the expected value and the output of the model split equally between them (since they equally contribute to the OR function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = [np.ones(M), np.zeros(M)]\n",
    "for x in xs:\n",
    "    print()\n",
    "    print(\"          x =\", x)\n",
    "    print(\"shap_values =\", shap.TreeExplainer(or_model).shap_values(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two feature XOR example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build data\n",
    "N = 100\n",
    "M = 4\n",
    "X = np.zeros((N,M))\n",
    "X.shape\n",
    "y = np.zeros(N)\n",
    "X[:N//2, 0] = 1\n",
    "X[:1 * N//4, 1] = 1\n",
    "X[N//2:3 * N//4, 1] = 1\n",
    "y[1 * N//4:N//2] = 1\n",
    "y[N//2:3 * N//4] = 1\n",
    "\n",
    "# fit model\n",
    "xor_model = sklearn.tree.DecisionTreeRegressor(max_depth=2)\n",
    "xor_model.fit(X, y)\n",
    "\n",
    "# draw model\n",
    "dot_data = sklearn.tree.export_graphviz(xor_model, out_file=None, filled=True, rounded=True, special_characters=True)  \n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explain the model\n",
    "\n",
    "Note that the bias term is the expected output of the model over the training dataset (0.5). The SHAP value for features not used in the model is always 0, while for $x_0$ and $x_1$ it is just the difference between the expected value and the output of the model split equally between them (since they equally contribute to the XOR function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = [np.ones(M), np.zeros(M)]\n",
    "for x in xs:\n",
    "    print()\n",
    "    print(\"          x =\", x)\n",
    "    print(\"shap_values =\", shap.TreeExplainer(xor_model).shap_values(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two feature AND + feature boost example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build data\n",
    "N = 100\n",
    "M = 4\n",
    "X = np.zeros((N,M))\n",
    "X.shape\n",
    "y = np.zeros(N)\n",
    "X[:N//2, 0] = 1\n",
    "X[:1 * N//4, 1] = 1\n",
    "X[N//2:3 * N//4, 1] = 1\n",
    "y[:1 * N//4] = 1\n",
    "y[:N//2] += 1\n",
    "\n",
    "# fit model\n",
    "and_fb_model = sklearn.tree.DecisionTreeRegressor(max_depth=2)\n",
    "and_fb_model.fit(X, y)\n",
    "\n",
    "# draw model\n",
    "dot_data = sklearn.tree.export_graphviz(and_fb_model, out_file=None, filled=True, rounded=True, special_characters=True)  \n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explain the model\n",
    "\n",
    "Note that the bias term is the expected output of the model over the training dataset (0.75). The SHAP value for features not used in the model is always 0, while for $x_0$ and $x_1$ it is just the difference between the expected value and the output of the model split equally between them (since they equally contribute to the AND function), plus an extra 0.5 impact for $x_0$ since it has an effect of $1.0$ all by itself (+0.5 if it is on and -0.5 if it is off)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = [np.ones(M), np.zeros(M)]\n",
    "for x in xs:\n",
    "    print()\n",
    "    print(\"          x =\", x)\n",
    "    print(\"shap_values =\", shap.TreeExplainer(and_fb_model).shap_values(x))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
